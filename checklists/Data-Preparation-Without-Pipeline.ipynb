{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "Dieses Notebook, dient lediglich zu Entwicklung von Vorverarbeitungsstrategien. Daher wird auf jedes Feature grob eingegangen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliiotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labelList=['symboling','normalizedLosses','make','fuelType','aspiration','numOfDoors','bodyStyle','driveWheels','engineLocation',\n",
    "           'wheelBase','length','width','height','curbWeight','engineType','numOfCylinders','engineSize','fuelSystem','bore',\n",
    "           'stroke','compressionRatio','horsepower','peakRpm','cityMpg','highwayMpg','price']\n",
    "df = pd.read_csv('../data/data_car-CopyForEDA.csv',delimiter=',',encoding='utf-8', names=labelList)\n",
    "df = df.replace(\"?\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.price.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## symboling\n",
    "laut First-Touch kein Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## price\n",
    "NAs werden duch den Druchschnittswert der Marke ausgewählt\n",
    "Der Datentyp wird in int gewandelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1810837475\\.conda\\envs\\Kompensationsarbeit\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "#df.price.isna().any()# > sind noch nas\n",
    "df[['price']]=df[['price']].astype('float64')\n",
    "for i in range(0, len(df.price),1):\n",
    "    if(str(df.price[i]) == \"nan\"):\n",
    "        nanmake = (df.make[i])\n",
    "        subSetNanMake=df[df['make'] == nanmake]\n",
    "        subSetNanMake = subSetNanMake.price.dropna()\n",
    "        medianPerMake = subSetNanMake.median(axis = 0, skipna = True)\n",
    "        df.price[i] = medianPerMake\n",
    "#df.price.isna().any() => keine Nas Mehr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numOfDoors\n",
    "nas werden durch den Modus ersetzt der entsprechenden Wagenklasse(bodyStyle). ich geh davon aus dass eine wagenklasse meist die gleiche Türenanzhal hat => ich setzt den Modus dazu rein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two' 'four' nan]\n"
     ]
    }
   ],
   "source": [
    "print(df.numOfDoors.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1810837475\\.conda\\envs\\Kompensationsarbeit\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "#df.numOfDoors.isna().any() #=> noch nas\n",
    "for i in range(0, len(df.numOfDoors),1):\n",
    "    if(str(df.numOfDoors[i]) == \"nan\"):\n",
    "        nanBodyStyle = (df.bodyStyle[i])\n",
    "        subSetNanBody=df[df['bodyStyle'] == nanBodyStyle]\n",
    "        subSetNanBody = subSetNanBody.numOfDoors.dropna()\n",
    "        modusNumOfDoors = subSetNanBody.mode().iloc[0]\n",
    "        #print(modusNumOfDoors)\n",
    "        df.numOfDoors[i] = modusNumOfDoors\n",
    "#df.numOfDoors.isna().any() # => keine Nas Mehr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der nachfolgede Teil zu numOfDoors ist aufgekommen, da im OHE ein außergewöhnlicher wert erschienen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two' 'four']\n"
     ]
    }
   ],
   "source": [
    "print(df.numOfDoors.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bore & Stroke & horse power & Peakrpm\n",
    "Das sind die Parameter, die den Hubraum(engine-Size) beschreiben. Nach ewig langer recherche errechne ich aber auch  bei Datensätzen bei welchen ich sowohl bore, stroke, horsepower & PRPM als auch engine habnciht den richtigen Hubrubraum. Somit geh ich anders vor.\n",
    "    1) finde Nonvalues in den bereichen (glück = bei Stroke&bore als auch bei HP&PRPM sind es jeweils die gleichen Datensätze)\n",
    "    2) wähle von nonvalue die Enigine size\n",
    "    3) finde die nächst kleinere Engine size\n",
    "    4) impute von den nächst kleineren die bore & Storke values dem leeren Feld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.bore.isna().any()\n",
    "df.stroke.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18, 61, '2.91', '3.03']]\n",
      "[[18, 61, '2.91', '3.03']]\n",
      "[[18, 61, '2.91', '3.03']]\n",
      "[[32, 79, '2.91', '3.07']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1810837475\\.conda\\envs\\Kompensationsarbeit\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\1810837475\\.conda\\envs\\Kompensationsarbeit\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Bore &Stroke\n",
    "from operator import itemgetter\n",
    "for i in range(0, len(df.stroke),1):\n",
    "    if(str(df.stroke[i]) == \"nan\"):\n",
    "        #print(i)\n",
    "        refferenceEngine = df.engineSize[i]\n",
    "        #print(refferenceEngine)\n",
    "        #finde alle die kleiner sind\n",
    "        possibleMachines = [] \n",
    "        for j in range(0, len(df),1):\n",
    "            if(df.engineSize[j]<refferenceEngine):\n",
    "                possibleMachines.append([j, df.engineSize[j], df.bore[j], df.stroke[j]])\n",
    "            else:\n",
    "                pass\n",
    "        #print(possibleMachines)\n",
    "        detectedValue=[possibleMachines[0]]# finde aus den möglichen Maschinen, die nächst kleinere\n",
    "        for j in range(0, len(possibleMachines),1):\n",
    "           \n",
    "            if(int(detectedValue[0][1]) >= int(possibleMachines[j][1])):\n",
    "                pass\n",
    "            else:\n",
    "                detectedValue = [possibleMachines[j]]\n",
    "        print(detectedValue)\n",
    "            \n",
    "        #jetzt den Update im Dataframe mit den broke & stroke von übergebenen DetectedValue (bei index = 0)\n",
    "        df.bore[i] = detectedValue[0][2]\n",
    "        df.stroke[i] = detectedValue[0][3]\n",
    "df[['bore']]=df[['bore']].astype('float64')\n",
    "df[['stroke']]=df[['stroke']].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.bore.isna().any()\n",
    "df.stroke.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 131, '140', '5500']]\n",
      "[[8, 131, '140', '5500']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1810837475\\.conda\\envs\\Kompensationsarbeit\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\1810837475\\.conda\\envs\\Kompensationsarbeit\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# HP&PRPM\n",
    "for i in range(0, len(df.horsepower),1):\n",
    "    if(str(df.horsepower[i]) == \"nan\"):\n",
    "        refferenceEngine = df.engineSize[i]\n",
    "        possibleMachines = [] \n",
    "        for j in range(0, len(df),1):\n",
    "            if(df.engineSize[j]<refferenceEngine):\n",
    "                possibleMachines.append([j, df.engineSize[j], df.horsepower[j], df.peakRpm[j]])\n",
    "            else:\n",
    "                pass\n",
    "        detectedValue=[possibleMachines[0]]\n",
    "        for j in range(0, len(possibleMachines),1): # finde aus den möglichen Maschinen, die nächst kleinere\n",
    "            if(int(detectedValue[0][1]) >= int(possibleMachines[j][1])):\n",
    "                pass\n",
    "            else:\n",
    "                detectedValue = [possibleMachines[j]]\n",
    "        print(detectedValue)\n",
    "            \n",
    "        #jetzt den Update im Dataframe mit den broke & stroke von übergebenen DetectedValue (bei index = 0)\n",
    "        df.horsepower[i] = detectedValue[0][2]\n",
    "        df.peakRpm[i] = detectedValue[0][3]\n",
    "#df.horsepower.isna().any()\n",
    "#df.peakRpm.isna().any()\n",
    "\n",
    "df[['horsepower']]=df[['horsepower']].astype('float64')\n",
    "df[['peakRpm']]=df[['peakRpm']].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OHE\n",
    "hier kann ich ein OHE verwenden"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "interim = one_hot_encoder.fit_transform(df.make.values.reshape(-1,1))\n",
    "interim = pd.DataFrame(interim.toarray(), columns=one_hot_encoder.categories_) \n",
    "df = pd.concat([df, interim], axis=1, sort=False)\n",
    "df = df.drop(['make'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfOHE=['make','fuelType','aspiration','numOfDoors','bodyStyle','driveWheels','engineLocation',\n",
    "           'engineType','numOfCylinders', 'fuelSystem']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "listOfOHE=['fuelType','aspiration']\n",
    "# bodystyle??'driveWheels',engineLocation',          'engineType','numOfCylinders', 'fuelSystem', 'numOfDoors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# show all Columns\\nfor col in df.columns: \\n    print(col)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "for feature in range(0,len(listOfOHE),1):\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    command = 'interim = one_hot_encoder.fit_transform(df.{}.values.reshape(-1,1))'.format(listOfOHE[feature])\n",
    "    exec(command)\n",
    "    interim = pd.DataFrame(interim.toarray(), columns=one_hot_encoder.categories_)\n",
    "    df = pd.concat([df, interim], axis=1, sort=False)\n",
    "    df = df.drop([listOfOHE[feature]], axis=1)\n",
    "    \n",
    "'''\n",
    "# show all Columns\n",
    "for col in df.columns: \n",
    "    print(col)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 76)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'./test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalized loss\n",
    "Ich versuche das mit einem Modell vorherzusagen, dazu muss aber die gesammelte andere Vorverarbeitung aller anderen Feature abgeschlossen sein. => das geschieht ganz zum schluss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) ergänzen von df um den Orginalindex, für die spätere rückordnung\n",
    "indexlist=list(range(0,len(df),1))\n",
    "dfIndex=pd.DataFrame(indexlist, columns=['OrginalIndex'])\n",
    "df = pd.concat([dfIndex, df], axis=1, sort=False)\n",
    "#split in die Werte die Nas(diese wollen wir durch das Modell predicten) haben und die die keine haben(mit diesen wollen wir modellieren)\n",
    "toBePredicted = []\n",
    "listForModelling = []\n",
    "for i in range(0,len(df),1):\n",
    "    #print(df.normalizedLosses[i])\n",
    "    if(str(df.normalizedLosses[i]) == \"nan\"):\n",
    "        interim = (list(df.iloc[i,]))\n",
    "        toBePredicted.append(interim)\n",
    "    else:\n",
    "        interim = (list(df.iloc[i,]))\n",
    "        listForModelling.append(interim)\n",
    "        \n",
    "titles = []\n",
    "for col in df.columns: \n",
    "    titles.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 77)\n",
      "(164, 77)\n"
     ]
    }
   ],
   "source": [
    "#Erstellen von DF spezifisch für die Anwendung modellierung bzw prediction\n",
    "DFtoBePredicted = pd.DataFrame(toBePredicted, columns = titles) \n",
    "print(DFtoBePredicted.shape)\n",
    "DFlistForModelling = pd.DataFrame(listForModelling, columns = titles) \n",
    "print(DFlistForModelling.shape) # <= auf dieses setzte ich jetzt ein Regressionsmodell an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164, 76)\n",
      "(164, 1)\n"
     ]
    }
   ],
   "source": [
    "# Horizontale aufteilung\n",
    "# über den Loop finde ich die Position von normalizedloss\n",
    "for i in range(0,len(DFlistForModelling),1):\n",
    "    #print(DFlistForModelling.columns[i])\n",
    "    if(str(DFlistForModelling.columns[i]) == \"normalizedLosses\"):\n",
    "        position=[i]\n",
    "        break\n",
    "        \n",
    "input_df = DFlistForModelling.drop(['normalizedLosses'], axis=1)\n",
    "print(input_df.shape)\n",
    "output_df = DFlistForModelling.iloc[:,position]\n",
    "print(output_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VertikaleAufteilung in test & Train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    input_df, \n",
    "    output_df,\n",
    "    test_size=0.2,\n",
    "    random_state = 90\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "             with_scaling=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### skallieren der Daten\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robust_scaler_X = RobustScaler()\n",
    "robust_scaler_y = RobustScaler()\n",
    "robust_scaler_X.fit(train_X)\n",
    "robust_scaler_y.fit(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_X = robust_scaler_X.transform(train_X)\n",
    "scaled_test_X = robust_scaler_X.transform(test_X)\n",
    "scaled_train_y = robust_scaler_y.transform(train_y)\n",
    "scaled_test_y = robust_scaler_y.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lineare Regression für die imputation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linerar_regressor = LinearRegression()\n",
    "linerar_regressor.fit(scaled_train_X, scaled_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = linerar_regressor.predict(scaled_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5719562994047731"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linerar_regressor.score(scaled_test_X, scaled_test_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicht übertreiben schlecht, ich verwende nun dieses Modell um die normalized loss von dem DF vorherzusagen verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vorbereiten der 2bePredicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [165.89382218]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "1 [170.49422248]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "2 [160.92547927]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "5 [182.16229395]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "7 [159.73009689]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "9 [194.04962037]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "14 [175.48231237]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "15 [176.21973396]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "16 [223.19326149]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "17 [175.42211649]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "43 [163.38053473]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "44 [122.47023326]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "45 [111.13520217]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "46 [178.23925346]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "48 [150.01967762]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "49 [142.82619175]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "63 [73.94212708]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "66 [137.06948693]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "71 [83.76013641]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "73 [81.53062671]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1810837475\\.conda\\envs\\Kompensationsarbeit\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3319: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "74 [119.17720362]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "75 [109.80242909]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "82 [154.41023056]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "83 [160.38535426]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "84 [160.3355124]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "109 [152.95761435]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "110 [156.63174614]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "113 [163.35613174]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "114 [164.28764604]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "124 [186.04617581]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "126 [154.80673442]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "127 [157.2681098]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "128 [144.57912795]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "129 [54.82703513]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "130 [97.18226256]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "131 [130.88173659]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "181 [110.43078876]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "189 [136.54506543]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "191 [113.26776958]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "192 [124.95992979]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n",
      "193 [144.68821139]\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_df = DFtoBePredicted.drop(['normalizedLosses'], axis=1)\n",
    "robust_scaler_X_toBepredicted = RobustScaler()\n",
    "scaled_X_toBepredicted = robust_scaler_X.transform(prediction_df)\n",
    "prediction2 = linerar_regressor.predict(scaled_X_toBepredicted)\n",
    "rescaledPredictions = robust_scaler_y.inverse_transform(prediction2) \n",
    "# zurückspeichern in \n",
    "#print(len(DFtoBePredicted))\n",
    "#print(len(rescaledPredictions))\n",
    "for i in range(0, len(DFtoBePredicted), 1):\n",
    "    print(DFtoBePredicted.OrginalIndex[i], rescaledPredictions[i])\n",
    "    print(df.normalizedLosses[DFtoBePredicted.OrginalIndex[i]])\n",
    "    df.normalizedLosses[DFtoBePredicted.OrginalIndex[i]] = rescaledPredictions[i]\n",
    "    print(df.normalizedLosses[DFtoBePredicted.OrginalIndex[i]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler_X_toBepredicted = RobustScaler()\n",
    "scaled_X_toBepredicted = robust_scaler_X.transform(train_X)\n",
    "\n",
    "#robust_scaler_y.inverse_transform(prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finaler Check\n",
    "hier schau ich lediglich ob iwo noch ein NA vorhanden ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
